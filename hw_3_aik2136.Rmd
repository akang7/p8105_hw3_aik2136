---
title: 'Homework #3'
author: "Ashley Kang"
date: "10/10/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(p8105.datasets)
```

## Problem 1
This problem uses the BRFSS data. First, do some data cleaning:

* Format the data to use appropriate variable names;
* Focus on the “Overall Health” topic
* Include only responses from “Excellent” to “Poor”
* Organize responses as a factor taking levels ordered from “Excellent” to “Poor”

```{r load_clean_data_1}
data(brfss_smart2010)

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) 
```

##### In 2002, which states were observed at 7 locations?
```{r states_location_1}
brfss_data_2002 = brfss_data %>%
  filter(year == "2002") %>%
  group_by(state) %>% 
  summarize(number_locations = n_distinct(county)) %>% 
  filter(number_locations == "7")
```

There are 3 states that were observed at 7 locations in 2002: Connecticut (CT), Florida (FL), and North Carolina (NC).

##### Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
```{r spaghetti_plot_1}
brfss_data %>% 
  group_by(year, state) %>% 
  summarize(number_locations = n()) %>% 
  ggplot(aes(x = year, y = number_locations, color = state)) +
  labs(
    title = "Number of locations in each state, 2002-2012",
    x = "Year",
    y = "Number of locations") +
  geom_line() +
  theme(legend.position = "right") +
  viridis::scale_color_viridis(name = "state", discrete = TRUE)
```

##### Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r mean_sd_1}
brfss_data %>% 
  filter(year == 2002 | year == 2006 | year == 2010, state == "NY") %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable()
```

##### For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r distribution_state_avg_1}
brfss_data %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            mean_very_good = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)) %>% 
  gather(key = mean_variable, value = mean_value, mean_excellent:mean_poor) %>% 
  ggplot(aes(x = year, y = mean_value, color = state)) +
  labs(
    title = "Distribution of state-level averages over time per response",
    x = "Year",
    y = "Average proportion") +
  geom_point() +
  geom_line() +
  facet_grid(~mean_variable) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(legend.position = "bottom")
```

## Problem 2
This problem uses the Instacart data.

##### Loading and cleaning Instacart data
```{r loading_cleaning_data_2}
instacart_data = instacart %>% 
  janitor::clean_names()

head(instacart_data)
tail(instacart_data)
```

The dataset `instacart` consists of **`r nrow(instacart_data)` rows** and **`r ncol(instacart_data)`** columns. Each row is a single product from an order and corresponds to information on the `product_name`, `grocery aisle`, `department`, if the product was `reordered`, day of the week and hour the product was ordered (`order_hour_of_day`), and the `user_id` who ordered it. There are **`r nrow(distinct(instacart_data, user_id))` distinct users** who ordered **`r nrow(distinct(instacart_data, product_id))` products** from **`r nrow(distinct(instacart_data, department_id))` distinct departments**.


##### How many aisles are there, and which aisles are the most items ordered from?
```{r aisles_2}
number_aisles = 
  instacart_data %>% 
  distinct(aisle_id) %>% 
  nrow()

instacart_data %>%
  group_by(aisle) %>%
  summarize(number_per_aisle = n()) %>%
  arrange(desc(number_per_aisle)) %>%
  head() %>%
  knitr::kable()
```

There are `r number_aisles` distinct aisles, of which the top 6 aisles with the most items ordered from are:

* Fresh vegetables - 150,609 items ordered
* Fresh fruits - 150,473 items ordered
* Packaged vegetables fruits - 78,493 items ordered
* Yogurt - 55,240 items ordred
* Packaged cheese - 41,699 items ordered
* Water seltzer sparkling water - 36,617 items ordered

#####  Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r plot_aisles_2}
instacart_data %>%
  group_by(aisle, department) %>%
  summarize(number_per_aisle = n()) %>%
  ungroup() %>%
  mutate(aisle = reorder(aisle, desc(number_per_aisle))) %>% 
  ggplot(aes(x = aisle, y = number_per_aisle, color = department, fill = department)) + 
  labs(
    title = "Number of items ordered per aisle",
    x = "Aisle",
    y = "Number of items") +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        legend.position = "bottom")
  
```

##### Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
```{r table_most_pop_aisles_2}
instacart_data %>% 
  filter(aisle == "Baking Ingredients" | aisle == "Dog Food Care" | aisle == "Packaged Vegetables Fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(number_per_aisle = n()) %>% 
  top_n(1, number_per_aisle) %>% 
  arrange(desc(number_per_aisle)) %>% 
  knitr::kable()
```

##### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r table_apple_coffee_2}
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_order_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_order_hour) %>% 
  setNames(c("Product", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) %>%
  knitr::kable()
```

## Problem 3

This problem uses the NY NOAA data.

```{r loading_data_3}
data(ny_noaa)

ny_noaa_data = ny_noaa %>% 
  janitor::clean_names()

head(ny_noaa_data)
tail(ny_noaa_data)

# Missing
missing_precip = ny_noaa_data %>%
  filter(is.na(prcp))

missing_snow = ny_noaa_data %>%
  filter(is.na(snow))

missing_snow_depth = ny_noaa_data %>%
  filter(is.na(snwd))

missing_tmax = ny_noaa_data %>%
  filter(is.na(tmax))

missing_tmin = ny_noaa_data %>%
  filter(is.na(tmin))

# Precipitation
avg_precip = mean(ny_noaa_data$prcp, na.rm = TRUE)

# Snowfall
avg_snow = mean(ny_noaa_data$snow, na.rm = TRUE)

# Snow-depth
avg_snow_depth = mean(ny_noaa$snwd, na.rm = TRUE)
```

The dataset `ny_noaa` contains **`r nrow(ny_noaa_data)` rows** and **`r ncol(ny_noaa_data)` columns**. Each row corresponds to a weather station's report from the corresponding date. The average amount of precipitation is **`r avg_precip` tenths of mm**. The average amount of snowfall is **`r avg_snow` mm**. The average snow depth is **`r avg_snow_depth` mm**. Missing data, particularly for variables `snwd`, `tmax`, and `tmin` is an issue in this dataset, as the proportion of missing `prcp` data is **`r nrow(missing_precip)/nrow(ny_noaa_data)*100`%**, missing `snow` data is **`r nrow(missing_snow)/nrow(ny_noaa_data)*100`%**, missing snow-depth data is **`r nrow(missing_snow_depth)/nrow(ny_noaa_data)*100`%**, missing `tmax` data is **`r nrow(missing_tmax)/nrow(ny_noaa_data)*100`%**, and missing `tmin` data is **`r nrow(missing_tmin)/nrow(ny_noaa_data)*100`%**. 

##### Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.
```{r clean_data_3}
ny_noaa_data_clean = ny_noaa_data %>%
  separate(date, into = c("year", "month", "date")) %>%
  mutate(prcp = prcp/10,
         tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10,
         year = as.numeric(year),
         month = as.numeric(month),
         date = as.numeric(date))
```

We are matching precipitation units to snowfall and snow depth (mm). We are changing tmax and tmin from tenths ºC to indicate decimals (ºC).

##### For snowfall, what are the most commonly observed values? Why?
```{r snowfall_3}
snowfall = as.data.frame(table(ny_noaa_data_clean$snow)) %>% 
  arrange(desc(Freq)) %>%
  rename(amount_snowfall = Var1) %>%
  head()

snowfall %>% knitr::kable()
```

The most commonly observed value for snowfall was 0 mm. Since it usually only snows during the winter (1 season out of 4) and even in the winter it does not snow every day, it would make sense that we observe 0 mm of snowfall for most of the year. 

##### Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r plot_average_temperature}
ny_noaa_data_clean %>%
  filter(!is.na(tmax), month == 1 | month == 7) %>% 
  mutate(month = month.name[month]) %>%
  group_by(id, year, month) %>%
  summarize(avg_tmax = mean(tmax)) %>%
  ggplot(aes(x = year, y = avg_tmax)) +
    labs(
    title = "Average maximum temperature in January vs. July across years",
    x = "Year",
    y = "Average maximum temperature"
  ) +
  geom_boxplot(aes(group = year)) +
  facet_grid(~month)
```

Yes, there is observable structure. There is a noticable difference in the maximum temperatures in January compared to July. The temperature values for January range from about -10 to 10ºC, while the values for July are about 20 to 30ºC. There appear to be more outliers for January, and these outliers tend to be for warmer temperatures. For July, the outliers are all below the median lines, which indicate that these outliers are for colder temperatures. This makes sense as it tends to be cold in January and hot in July in New York.

##### Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r two_panel_plot_3}
tmax_tmin_hex = ny_noaa_data_clean %>%
  ggplot(aes(x = tmax, y = tmin)) + 
  labs(
    title = "Hex plot of max vs min temperature",
    x = "Max temperature (ºC)",
    y = "Min temperature (ºC)"
  ) +
  geom_hex() + 
  theme(legend.text = element_text(hjust = 1))

dist_snowfall_values = ny_noaa_data_clean %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow)) + 
  labs(
    title = "Boxplot of snowfall per year",
    x = "Year",
    y = "Snowfall (mm)"
  ) +
  geom_boxplot(aes(group = year))

tmax_tmin_hex + dist_snowfall_values
```

Looking at the boxplot, 1998, 2006, and 2010 display outliers, with 2006 showing the most outliers for snowfall
